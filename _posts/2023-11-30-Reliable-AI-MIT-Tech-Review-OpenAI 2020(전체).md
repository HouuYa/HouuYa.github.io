---
title: "[MIT Technolgy Reviews] The messy, secretive reality behind OpenAI’s bid to save the world 1부"
categories:
  - AI Regulation
tags:
  - AI
  - MIT Technolgy Reviews
  - Reliable AI
#  - AI policy
#  - news scraps
  - 인공지능
  - 안전
  - 신뢰
  - 정책
  - OpenAI

last_modified_at: 2023-11-30T10:56:11-10:30
---
_________________

ARTIFICIAL INTELLIGENCE

# The messy, secretive reality behind OpenAI’s bid to save the world

### The AI moonshot was founded in the spirit of transparency. This is the inside story of how competitive pressure eroded that idealism.

#### By Karen Hao
#### February 17, 2020
---


  * MIT Technolgy Reviews에 2020년 2월 17일, AI분야 기자 [Karen Hao](https://www.technologyreview.com/author/karen-hao/)이 작성한 [기사](https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/)를 DeepL로 번역 후, "포괄적요약"과 "저의 의견"을 작성한 내용
  * 2022년 ChatGPT로 일반인들(나 포함)에게 알려지기 까지, 2019년 2020년 초에 OpenAI가 어떤 기업이였는지 알 수 있는 자료
  * 만약 저작권에 문제가 있다면 바로 알려주세요!!

---

> 2015년 설립 당시, AI 개발과 실행에 드는 비용때문에 소수에게 권력이 집중되는 경향을 예측하여, 비영리기관이라는 업계의 이목을 집중시켜, 경쟁사와는 다른 출발한 것에 대해 실리콘밸리 사람들의 대단한 예측력과 사업추진력을 알수 있음
>
> 그리고 startup 기업이 초창기 목표와 비전을 지켜나가기 위해서 어떤 갈등이 있고, 어떻게 해쳐나가는지도 느껴질 수 있음
> 
> 우리나라도 그간 중공업과 건설 발전 사례 등처럼 국가차원과 이제 세계적인 기업들차원에서 선택과 집중이 필요 시점이 아닐까 한다
    

---
### OpenAI 개략 소개

매년 OpenAI의 직원들은 인공 일반 인공지능(AGI)이 언제쯤 도래할 것이라고 생각하는지 투표
절반은 15년 이내에 실현될 것으로 예상

OpenAI는 설립 4년이라는 짧은 기간 동안 세계 최고의 AI 연구소 중 하나
지속적으로 헤드라인을 장식하는 연구를 수행하며 명성을 쌓았고, 엘론 머스크와 전설적인 투자자 샘 앨트먼이 설립자 중 한 명으로 꼽히는 등 실리콘 밸리에서 많은 사랑을 받고 있음

회사의 목표는 인간 정신의 학습 및 추론 능력을 갖춘 기계인 AGI를 최초로 개발하되, 이 기술을 안전하게 개발하고 그 혜택을 전 세계에 골고루 분배하는 것

이는 기술 개발이 저항이 가장 적은 경로를 따르도록 내버려두면 AGI가 쉽게 혼란에 빠질 수 있다는 것을 의미

이제 우리는 알고리즘이 편향되고 취약하며, 엄청난 남용과 기만을 저지를 수 있고, 알고리즘을 개발하고 실행하는 데 드는 비용 때문에 소수의 손에 권력이 집중되는 경향

이러한 상황에서 OpenAI는 부유한 기업들이 지배하는 이 분야에서 비영리 단체로 설립되었고, 첫 번째 발표 내용은 "주주가 아닌 모든 사람을 위한 가치를 창출"

직원들의 급여가 결정될 정도로 중요한 이 [헌장](https://openai.com/charter/)에는 OpenAI의 "일차적 신탁 의무는 인류에 대한 것"이라고 선언

AGI를 안전하게 달성하는 것은 매우 중요하기 때문에, 다른 조직이 AGI를 먼저 달성하는 데 가까워지면 OpenAI는 경쟁을 중단하고 대신 협력할 것이라는 것은 투자자와 언론에 잘 먹혔고, 7월에 Microsoft가 10억 달러를 투자
<!-- 
<figure>
    <img src="https://cdn.technologyreview.com/i/images/cf3a2394.jpg?sw=700&cx=0&cy=0&cw=3000&ch=2000"
         alt="OpenAI's logo hanging in its office">
    <figcaption> <br> OpenAI's logo hanging in its office.</br> Christie Hemm Klok </figcaption>
</figure>

하지만 3일 동안 OpenAI의 사무실에서 전현직 직원, 협력자, 친구, 이 분야의 다른 전문가들과 진행한 인터뷰를 진행한 결과, 회사가 공개적으로 지지하는 것과 비공개적으로 운영되는 방식 사이에는 불일치가 존재

시간이 지남에 따른 치열한 경쟁과 더 많은 자금 조달 압박 때문에 투명성, 개방성, 협업이라는 창립 이념이 약화



---
### OpenAI 등장 배경

초창기 개념이 정립된 이래 AI 분야는 인간과 유사한 지능을 이해하고 이를 재현하기 위해 노력해 왔습니다. 1950년 영국의 저명한 수학자이자 컴퓨터 과학자였던 앨런 튜링은 "기계도 생각할 수 있을까?"라는 지금은 유명한 도발적인 질문으로 [논문](https://www.csee.umbc.edu/courses/471/papers/turing.pdf)을 시작했습니다. 6년 후, 이 도발적인 아이디어에 매료된 과학자들이 다트머스 대학에 모여 이 학문을 공식화하기 시작했습니다.

시애틀에 본사를 둔 비영리 인공지능 연구소인 앨런 인공지능 연구소(AI2)의 CEO인 오렌 에치오니는 "이는 모든 지적 역사에서 가장 근본적인 질문 중 하나죠?"라고 말합니다. "우주의 기원을 이해하고 있는가? 물질을 이해하고 있는가?"

문제는 인공지능이 항상 모호한 상태로 남아 있다는 것입니다. 어떤 모습일지, 최소한 어떤 일을 해야 하는지 아무도 설명할 수 없습니다. 예를 들어, 일반 지능이 한 가지 종류만 있다는 것은 분명하지 않으며, 인간 지능은 그 하위 집합에 불과할 수 있습니다. 또한 AGI가 어떤 용도로 사용될 수 있는지에 대한 의견도 다양합니다. 좀 더 낭만적인 관점에서 보면, 수면의 필요성이나 인간 의사소통의 비효율성에 방해받지 않는 기계 지능은 기후 변화, 빈곤, 기아 같은 복잡한 문제를 해결하는 데 도움이 될 수 있습니다.

그러나 이러한 고급 기능을 개발하는 데는 수십 년, 심지어는 수 세기가 걸릴 수도 있다는 것이 이 분야의 공통된 의견입니다. 또한 많은 사람들은 이러한 목표를 지나치게 열성적으로 추구하면 역효과를 낼 수 있다고 우려합니다. 1970년대에 이어 80년대 말과 90년대 초에도 이 분야는 과잉 약속과 과소 제공을 반복했습니다. 하룻밤 사이에 자금이 고갈되어 한 세대의 연구자들에게 깊은 상처를 남겼습니다. 최근까지 OpenAI가 회원사로 있는 산업 그룹 파트너십 온 AI의 연구 책임자였던 피터 에커슬리는 "이 분야는 마치 불모지처럼 느껴졌습니다."라고 말합니다.



<figure>
    <img src="https://cdn.technologyreview.com/i/images/cf3a2415.jpg?sw=700&cx=0&cy=0&cw=2100&ch=1400"
         alt="A conference room">
    <figcaption> <br> A conference room on the first floor named Infinite Jest.</br> Christie Hemm Klok </figcaption>
</figure>

---

이러한 배경에서 2015년 12월 11일, OpenAI는 세상에 화려하게 등장했습니다. 오픈AI가 공개적으로 AGI를 추구한다고 선언한 것은 딥마인드가 처음이 아니며, 5년 전인 2014년에 구글에 인수된 바 있습니다. 하지만 OpenAI는 달랐습니다. 머스크, 알트만, 페이팔의 공동 창업자 피터 틸을 비롯한 개인 투자자들이 10억 달러를 투자해 벤처를 시작한다는 파격적인 가격부터가 충격적이었습니다.

스타들이 대거 참여한 투자자 명단은 미디어의 열광을 불러일으켰고, 인상적인 초기 직원 명단도 화제가 되었습니다: 결제 회사인 Stripe에서 기술을 담당했던 그렉 브록먼이 최고기술책임자로, AI의 선구자 제프리 힌튼에게 수학한 일리아 수츠케버가 연구 책임자로, 그리고 일류 대학을 갓 졸업했거나 다른 회사에서 영입한 7명의 연구원이 핵심 기술팀을 구성하게 됩니다. (지난 2월(2019년), 머스크는 회사의 방향성에 대한 의견 불일치로 테슬라를 떠나겠다고 발표했습니다. 한 달 후, [알트먼](https://en.wikipedia.org/wiki/Sam_Altman)은 스타트업 액셀러레이터인 Y 컴비네이터의 사장직에서 물러나 OpenAI의 CEO가 되었습니다.)

하지만 무엇보다도 OpenAI가 비영리 단체라는 점이 눈에 띄었습니다. "자신의 이익보다 모두를 위한 좋은 결과를 우선시할 수 있는 선도적인 연구 기관을 보유하는 것이 중요할 것입니다."라고 발표문에서 밝혔습니다. "연구자들은 논문, 블로그 게시물, 코드 등 연구 결과를 발표하도록 강력히 장려할 것이며, 특허가 있다면 전 세계와 공유할 것입니다." 명시적으로 비판을 하지는 않았지만, 딥마인드와 같은 다른 연구소가 상업적 이해관계에 얽매여 인류를 위해 봉사할 수 없다는 의미는 분명했습니다. 그들이 문을 닫는 동안 OpenAI는 문을 열었습니다.

점점 더 사유화되고 단기적인 재정적 이익에 집중하는 연구 환경에서 OpenAI는 가장 큰 문제를 해결하는 데 필요한 자금을 조달할 수 있는 새로운 방법을 제시하고 있었습니다. "희망의 등불이었습니다."라고 머신러닝 전문가인 칩 후이엔은 말합니다.


---

샌프란시스코 18번가와 폴섬 스트리트의 교차로에 위치한 OpenAI의 사무실은 신비로운 창고처럼 보입니다. 유서 깊은 이 건물은 칙칙한 회색 판넬과 틴티드 창문으로 이루어져 있으며 대부분의 블라인드가 내려져 있습니다. 과거 소유주였던 파이오니어 트럭 팩토리의 흔적인 "PIONEER BUILDING"이라는 글자가 빛바랜 빨간색 페인트로 모퉁이를 감싸고 있습니다.

내부는 밝고 통풍이 잘되는 공간입니다. 1층에는 몇 개의 공용 공간과 두 개의 회의실이 있습니다. 한 곳은 대규모 회의를 진행하기에 적당한 크기인 스페이스 오디세이, 다른 한 곳은 전화 부스처럼 꾸며진 인피니트 제스트라고 불립니다. 이곳은 제가 방문하는 동안 출입이 제한되는 공간입니다. 2층과 3층에는 모든 사람의 책상과 여러 대의 로봇, 그리고 거의 모든 흥미로운 물건이 있습니다. 인터뷰 시간이 되면 사람들이 저에게 내려옵니다. 한 직원이 회의 중간중간 저를 주시합니다.


<figure>
    <img src="https://cdn.technologyreview.com/i/images/pioneertrunkfactory-c-a-malmco-2012-09-0115-43-33-2.jpg?sw=700&cx=0&cy=0&cw=2560&ch=1838"
         alt="The Pioneer Building">
    <figcaption> <br> The Pioneer Building.</br> wikimedia commons / tfinc </figcaption>
</figure>


파란 하늘이 아름다운 날 브록먼을 만나러 도착한 날, 그는 긴장한 듯 경계하는 표정이었습니다. "이렇게 많은 액세스 권한을 부여한 적은 처음입니다."라고 그는 살짝 미소를 지으며 말합니다. 그는 캐주얼한 옷차림에 OpenAI의 다른 직원들과 마찬가지로 효율적이고 군더더기 없는 사고방식을 반영하는 듯 덥수룩한 헤어스타일을 하고 있습니다.

올해 31세인 브록먼은 노스다코타의 취미 농장에서 자랐으며, "집중력 있고 조용한 어린 시절"을 보냈다고 설명합니다. 소의 젖을 짜고, 달걀을 모으고, 독학으로 공부하면서 수학의 매력에 푹 빠졌습니다. 2008년 수학과 컴퓨터 공학을 복수 전공할 생각으로 하버드에 입학했지만, 현실 세계에 뛰어들기에는 금세 불안해졌습니다. 그는 1년 후 자퇴하고 대신 MIT에 입학했다가 몇 달 만에 다시 자퇴했습니다. 두 번째는 그의 결정이 최종적이었습니다. 샌프란시스코로 이사한 후 그는 뒤도 돌아보지 않았습니다.

브록먼은 회사 전체 회의가 열리는 동안 저를 사무실로 데려가 점심을 먹게 했습니다. 길 건너편 카페에서 그는 강렬하고 진지하게, 그리고 경이로움에 가득 찬 목소리로 OpenAI에 대해 이야기하면서 종종 그 사명과 과학사의 획기적인 업적 사이의 유사점을 이끌어 냈습니다. 리더로서 그의 카리스마를 쉽게 느낄 수 있습니다. 그는 자신이 읽은 책에서 기억에 남는 구절을 이야기하면서 밸리에서 가장 좋아하는 이야기인 미국의 달 탐사 경쟁에 대해 집중적으로 설명합니다. ("제가 정말 좋아하는 이야기 중 하나는 청소부 이야기입니다."라고 그는 유명하지만 아마도 위서일지도 모르는 이야기를 언급하며 말합니다. "케네디가 청소부에게 다가가 '뭐 하는 거냐'고 묻자 청소부는 '사람을 달에 보내는 일을 돕고 있습니다'라고 대답했습니다.") 또한 대륙 횡단 철도("실제로는 전적으로 수작업으로 이루어진 마지막 거대 프로젝트였고... 엄청난 규모의 프로젝트였기 때문에 완전히 위험했습니다.")와 토머스 에디슨의 백열전구("저명한 전문가들로 구성된 위원회가 '절대 성공할 수 없다'고 말했지만 1년 후 그는 출하했습니다.")도 있습니다.

<figure>
    <img src="https://cdn.technologyreview.com/i/images/cf3a2258final.jpg?sw=700&cx=133&cy=611&cw=1792&ch=2389"
         alt=" Greg Brockman">
    <figcaption> <br> Greg Brockman, co-founder and CTO.</br> Christie Hemm Klok / tfinc </figcaption>
</figure>

브록먼은 OpenAI가 얼마나 위험한 도박을 하고 있는지, 그리고 그것이 냉소주의와 의심의 눈초리를 불러일으킨다는 사실을 잘 알고 있습니다. 하지만 그가 전하는 메시지는 분명합니다. 사람들은 얼마든지 회의적일 수 있다는 것입니다. 대담한 도전의 대가입니다.

---
### OpenAI 등장 이후 최근까지


초창기 OpenAI에 합류한 사람들은 그 에너지와 흥분, 목적 의식을 기억합니다. 긴밀한 연결망을 통해 소규모로 구성된 팀이었으며, 경영진은 느슨하고 비공식적으로 운영되었습니다. 모두가 누구든 아이디어와 토론을 할 수 있는 평등한 구조를 믿었습니다.

집단적 신화를 구축하는 데 머스크의 역할도 적지 않았습니다. "그가 저에게 제시한 방식은 '이봐요, 알겠어요. 하지만 만약 그렇지 않다면?"이라는 식이었습니다." UC 버클리의 피터 애빌 교수는 처음 2년 동안 여러 학생들과 함께 그곳에서 일했다고 회상합니다. "'향후 5년에서 10년 내에 이런 일이 일어날 확률이 1% 또는 0.1%라도 된다면 어떨까요? 아주 신중하게 생각해야 하지 않을까요? 그 말에 공감이 갔습니다."라고 그는 말합니다.

하지만 이러한 비공식성은 방향성의 모호함으로 이어지기도 했습니다. 2016년 5월, 알트먼과 브록먼은 당시 Google 연구원이었던 다리오 아모데이의 방문을 받았는데, 아모데이는 아무도 자신들이 하는 일을 이해하지 못한다고 말했습니다. 뉴요커에 실린 기사에 따르면, 팀 자체도 이 사실을 잘 모르고 있었다고 합니다. 브록먼은 "지금 우리의 목표는... 최선을 다하는 것입니다."라고 말합니다. "조금 모호합니다."

그럼에도 불구하고 아모데이는 몇 달 후 팀에 합류했습니다. 그의 여동생인 다니엘라 아모데이는 이전에 브록맨과 함께 일한 적이 있었고, 이미 OpenAI의 많은 구성원을 알고 있었기 때문입니다. 2년 후, 브록먼의 요청으로 다니엘라도 합류했습니다. "우리가 아무것도 없이 시작했다고 상상해 보세요."라고 브록먼은 말합니다. "우리는 그저 AGI가 잘 되었으면 좋겠다는 이상만 가지고 있었습니다."

### Throughout our lunch, Brockman recites the charter like scripture, an explanation for every aspect of the company’s existence.

15개월이 지난 2017년 3월, 경영진은 이제 더 집중해야 할 때라는 것을 깨달았습니다. 그래서 브록맨과 몇몇 핵심 멤버들은 AGI로 가는 길을 제시하는 내부 문서 초안을 작성하기 시작했습니다. 하지만 이 과정에서 치명적인 결함이 금방 드러났습니다. 팀은 이 분야의 트렌드를 연구하면서 비영리 단체로 남는 것이 재정적으로 불가능하다는 사실을 깨달았습니다. 이 분야의 다른 사람들이 획기적인 결과를 얻기 위해 사용하는 컴퓨팅 리소스는 3.4개월마다 두 배씩 증가하고 있었습니다. 브록먼은 "관련성을 유지하려면" 이러한 기하급수적인 증가에 대응하거나 이를 뛰어넘을 수 있는 충분한 자본이 필요하다는 것이 분명해졌다고 말합니다. 그러기 위해서는 빠르게 자금을 모으는 동시에 미션에 충실할 수 있는 새로운 조직 모델이 필요했습니다.

일반 대중과 대부분의 직원들은 알지 못하지만, 2018년 4월에 OpenAI는 이를 염두에 두고 헌장을 발표했습니다. 이 문서에는 연구소의 핵심 가치를 다시 표현하되, 새로운 현실을 반영해 표현을 미묘하게 바꿨습니다. "인류를 해치거나 권력을 과도하게 집중시키는 AI 또는 AGI의 사용을 피한다"는 약속과 함께 자원의 필요성도 강조했습니다. "우리의 사명을 완수하기 위해 상당한 자원을 동원해야 할 것으로 예상한다"면서도 "광범위한 이익을 훼손할 수 있는 직원과 이해관계자 간의 이해 상충을 최소화하기 위해 항상 성실히 행동할 것"이라고 밝혔습니다.

브록먼은 "회사 전체가 일련의 원칙을 받아들이도록 하기 위해 내부적으로 직원들과 오랜 시간 반복적으로 논의했습니다."라고 말합니다. "회사의 구조가 바뀌어도 변하지 않아야 하는 것들이었습니다."

<figure>
    <img src="https://cdn.technologyreview.com/i/images/cf3a2221final.jpg?sw=700&cx=0&cy=0&cw=3000&ch=1688"
         alt=" From left to right">
    <figcaption> <br>From left to right: Daniela Amodei, Jack Clark, Dario Amodei, Jeff Wu (technical staff member), Greg Brockman, Alec Radford (technical language team lead), Christine Payne (technical staff member), Ilya Sutskever, and Chris Berner (head of infrastructure).</br> Christie Hemm Klok </figcaption>
</figure>


이러한 구조 변화는 2019년 3월에 이루어졌습니다. OpenAI는 비영리 단체의 일부인 이사회가 감독하긴 하지만 투자자의 수익을 100배로 제한하는 영리 법인인 '수익 상한제' 부서를 설립함으로써 순수 비영리 단체의 지위를 박탈했습니다. 얼마 지나지 않아 Microsoft가 10억 달러를 투자한다고 발표했습니다(이 금액이 현금과 Microsoft의 클라우드 컴퓨팅 플랫폼인 Azure에 대한 크레딧으로 나뉜다는 사실은 밝히지 않았습니다).

예상대로 이러한 움직임은 OpenAI가 본연의 임무에서 벗어나고 있다는 비난의 물결을 일으켰습니다. 발표 직후 Hacker News에 올라온 게시물에서 한 사용자는 100배 제한이 어떻게 제한이 될 수 있는지 물었습니다. "구글의 초기 투자자들은 자본금 대비 약 20배의 수익을 얻었습니다."라고 그들은 썼습니다. "구글보다 몇 배나 더 많은 수익을 내는 기업 구조를 갖게 될 텐데 '과도한 권력 집중'을 원하지 않는다고요? 어떻게 할 수 있을까요? 자원의 집중이 아니라면 권력이란 정확히 무엇일까요?"

이러한 움직임에 많은 직원들도 비슷한 우려를 표명했습니다. 내부의 불안을 잠재우기 위해 경영진은 철저한 보안이 보장된 일련의 전환 문서의 일부로 FAQ를 작성했습니다. "OpenAI를 믿어도 되나요?" 한 질문이 있었습니다. "예"라는 대답과 함께 설명이 이어졌습니다.


---

[헌장](https://openai.com/charter/)은 OpenAI의 근간입니다. 헌장은 연구소의 모든 전략과 행동의 발판 역할을 합니다. 점심 식사 내내 브록먼은 회사의 모든 측면을 설명하는 경전처럼 이 헌장을 낭독했습니다. (한 대사를 암송하는 도중에 그는 "그런데 이 모든 대사를 정확히 알아듣기 위해 많은 시간을 들여 꼼꼼히 살펴봤기 때문에 잘 알고 있는 것 같다"고 설명합니다. 회의 전에 이 글을 읽었던 것과는 전혀 달라요.")

> ***더 발전된 역량을 개발하는 과정에서 인간이 의미 있는 삶을 지속할 수 있도록 어떻게 보장할 것인가요?(How will you ensure that humans continue to live meaningful lives as you develop more advanced capabilities?)*** -> "모든 사람에게 경제적 자유를 주고, 현재로서는 상상할 수 없는 새로운 기회를 찾을 수 있도록 하는 것이 그 영향력이 되어야 한다고 생각합니다." 
> 
> ***AGI를 균등하게 분배하기 위해 어떻게 조직을 구성할 것인가요?(How will you structure yourself to evenly distribute AGI?)***  -> "유틸리티는 우리가 가진 비전을 가장 잘 비유한 것이라고 생각합니다. 하지만 이 모든 것은 헌장의 적용을 받습니다." 
> 
> ***안전을 저해하지 않으면서 AGI에 먼저 도달하기 위해 어떻게 경쟁하나요?(How do you compete to reach AGI first without compromising safety?)*** -> "균형을 잡는 것이 중요하다고 생각하며, 이를 위한 최선의 방법은 헌장에 명시된 것입니다."

Brockman에게 있어 문서에 대한 엄격한 준수는 OpenAI의 구조를 작동시키는 원동력입니다. 모든 정규직 직원은 몇 가지 예외를 제외하고는 같은 사무실에서 근무해야 할 정도로 내부 조율을 가장 중요하게 여깁니다. 정책팀, 특히 디렉터인 Jack Clark에게는 샌프란시스코와 워싱턴 DC를 오가는 생활이 필수적입니다. Clark은 이런 생활 방식에 전혀 개의치 않습니다. 그는 동료들과 함께 하는 점심시간과 같은 중간중간의 순간이 모두가 같은 생각을 공유할 수 있도록 도와준다고 말합니다.

여러 면에서 이 접근 방식은 분명히 효과가 있습니다. 이 회사는 인상적으로 획일적인 문화를 가지고 있습니다. 직원들은 장시간 근무하고 식사 및 사교 시간을 통해 업무에 대해 끊임없이 이야기하며, 많은 직원이 같은 파티에 참석하고 "효과적인 이타주의"라는 합리적 철학을 신봉합니다. 이들은 머신러닝 용어를 사용해 농담을 주고받으며 자신의 삶을 설명합니다: "당신의 삶은 무엇의 함수입니까?" "당신은 무엇을 위해 최적화하고 있나요?" "모든 것은 기본적으로 최소함수입니다." 공정하게 말하자면, 다른 AI 연구자들도 이런 일을 좋아하지만, OpenAI를 잘 아는 사람들은 이 분야의 다른 사람들보다 직원들이 AI 연구를 직업이 아니라 정체성으로 여긴다고 말합니다. (지난 11월, 브록먼은 1년 동안 사귄 여자친구 안나와 사무실에서 OpenAI 로고로 장식된 꽃을 배경으로 결혼식을 올렸습니다. 주례는 서츠케버가 맡았고, 반지를 든 로봇 손이 반지를 전달했습니다.)


하지만 작년 중반 어느 순간, 헌장은 점심시간의 대화 소재 이상의 의미를 갖게 되었습니다. 수익 상한제로 전환한 직후, 경영진은 부분적으로 각 직원의 사명 몰입도를 기준으로 새로운 급여 체계를 도입했습니다. '통합 기술 사다리'라는 제목의 스프레드시트 탭에 '엔지니어링 전문성', '연구 방향' 등의 열과 함께 마지막 열에는 각 직급에 대한 문화 관련 기대치가 요약되어 있습니다. 레벨 3: "OpenAI 헌장을 이해하고 내재화합니다." 레벨 5: "자신과 팀원이 참여하는 모든 프로젝트가 헌장에 부합하는지 확인합니다." 레벨 7: "헌장을 준수하고 개선하며, 조직 내 다른 사람에게도 동일한 책임을 부여할 책임이 있습니다."


대부분의 사람들이 OpenAI에 대해 처음 들어본 것은 2019년 2월 14일이었습니다. 그날 연구소는 버튼 하나만 누르면 설득력 있는 에세이와 기사를 생성할 수 있는 모델이라는 인상적인 새 연구를 발표했습니다. 반지의 제왕에 나오는 문장이나 마일리 사이러스의 절도 행각에 관한 (가짜) 뉴스의 첫 문장을 입력하면 같은 맥락의 텍스트를 여러 문단 이어서 뱉어내는 것이었습니다.

하지만 연구원들은 GPT-2라고 불리는 이 모델은 출시하기에는 너무 위험하다고 말했습니다. 이러한 강력한 기술이 악의적인 사람의 손에 들어가면 엄청난 규모의 허위 정보를 생산하기 위해 쉽게 무기화될 수 있습니다.

과학자들 사이에서는 즉각적인 반발이 일어났습니다. 일부 과학자들은 OpenAI가 홍보를 위한 쇼를 벌이고 있다고 말했습니다. GPT-2는 위협이 될 만큼 충분히 발전하지 않았다는 것이었습니다. 만약 위협이 된다면 왜 그 존재를 발표하고 대중의 조사를 막았을까? AI로 생성된 허위 정보를 연구하는 럿거스 대학교의 조교수인 브릿 패리스(Britt Paris)는 "OpenAI가 AI에 대한 공포를 이용하려는 것 같았다"고 말합니다.

<figure>
    <img src="https://cdn.technologyreview.com/i/images/cf3a2285final-web.jpg?sw=700&cx=30&cy=411&cw=1940&ch=2587"
         alt="Jack Clark">
    <figcaption> <br> Jack Clark, policy director.</br> Christie Hemm Klok </figcaption>
</figure>

5월이 되자 OpenAI는 입장을 수정하여 "단계적 출시" 계획을 발표했습니다. 그 후 몇 달에 걸쳐 점점 더 강력한 버전의 GPT-2를 순차적으로 출시했습니다. 그 사이에도 여러 연구 기관과 협력하여 알고리즘의 악용 가능성을 면밀히 조사하고 대응책을 개발했습니다. 마침내 11월에 전체 코드를 공개하면서 "아직까지 오용의 강력한 증거는 발견되지 않았다"고 밝혔습니다.

대중의 관심을 끌기 위한 것이라는 비난이 계속되는 가운데 OpenAI는 GPT-2가 스턴트가 아니라고 주장했습니다. 오히려 일련의 내부 논의와 토론을 거쳐 합의된 신중한 실험이었다고 주장했습니다. 이번에는 다소 과한 행동이었더라도 앞으로 더 위험한 연구를 처리하는 데 선례가 될 것이라는 데 의견이 일치했습니다. 게다가 헌장에서는 "안전과 보안 문제"로 인해 연구소가 "앞으로 전통적인 출판을 점차 줄여야 할 것"이라고 예측했습니다.

이는 제가 회의에 참석했을 때 정책팀이 6개월에 걸친 후속 블로그 게시물에서 신중하게 설명한 주장이기도 합니다. 정책 연구 과학자인 마일스 브런디지는 Google 문서에 있는 내용을 강조하며 "이것이 바로 성공 사례 프레임워크의 일부라고 생각합니다."라고 말했습니다. "이 섹션의 선두에 서야 합니다: "우리는 야심 찬 일을 해냈고, 이제 몇몇 사람들이 그것을 모방하고 있으며, 그것이 유익했던 몇 가지 이유가 있습니다."

그러나 GPT-2를 이용한 OpenAI의 미디어 캠페인은 광범위한 AI 커뮤니티를 불안하게 만드는 잘 정립된 패턴을 따르기도 했습니다. 지난 몇 년 동안 이 연구소의 크고 화려한 연구 발표는 AI 과대광고 주기를 부추긴다는 비난을 반복적으로 받아왔습니다. 또한 비평가들은 이 연구소가 연구 결과를 지나치게 부풀렸다는 비난을 여러 차례 받기도 했습니다. 이러한 이유로 이 분야의 많은 사람들은 OpenAI를 멀리하는 경향이 있습니다.

<figure>
    <img src="https://cdn.technologyreview.com/i/images/cf3a2404.jpg?sw=700&cx=0&cy=0&cw=3000&ch=2000"
         alt="OpenAI office wall">
    <figcaption> <br> Cover images of OpenAI's research releases hang on its office wall.</br> Christie Hemm Klok </figcaption>
</figure>

그렇다고 해서 연구소가 대외적인 이미지 제고에 지속적으로 자원을 투입하는 것을 멈추지 않았습니다. 연구 논문은 물론, 글쓰기부터 멀티미디어 제작, 표지 이미지 디자인까지 모든 것을 자체적으로 제작하는 회사 블로그 게시물을 통해 연구 결과를 발표하고 있습니다. 한때는 딥마인드의 알파고에 관한 90분짜리 영화에 버금가는 다큐멘터리를 제작하기 위해 프로젝트 중 하나에 대한 다큐멘터리 개발에 착수하기도 했습니다. 결국 이 작업은 독립 제작으로 이어졌고, 현재 브록맨과 그의 아내 Anna가 부분적으로 자금을 지원하고 있습니다. (저는 다큐멘터리에 출연하여 OpenAI의 성과에 대한 기술적 설명과 맥락을 제공하는 데 동의했습니다. 이에 대한 보상은 받지 않았습니다.)

반발이 커지면서 이를 해결하기 위한 내부 논의도 진행 중입니다. 직원들은 외부의 끊임없는 비판에 좌절감을 느꼈고, 경영진은 이로 인해 연구소의 영향력과 최고의 인재를 채용할 수 있는 능력이 약화될 것을 우려했습니다. 내부 문서에는 이 문제와 이를 해결하기 위한 홍보 전략이 나와 있습니다: "정부 차원의 정책적 영향력을 가지려면 ML[머신러닝] 연구와 AGI에 대해 가장 신뢰할 수 있는 출처로 여겨져야 합니다."라고 '정책' 섹션의 한 줄에 적혀 있습니다. "연구 커뮤니티의 광범위한 지원과 지지는 그러한 평판을 얻는 데 필요할 뿐만 아니라 우리의 메시지를 증폭시킬 것입니다." 또 다른 항목은 '전략' 아래에 "ML 커뮤니티를 커뮤니케이션 이해관계자로 명시적으로 취급합니다. 우리가 의도적으로 선택한 경우에만 커뮤니티를 적대시하도록 어조와 외부 메시지를 변경합니다."


---

GPT-2가 그토록 격렬한 반발을 불러일으킨 데에는 또 다른 이유가 있었습니다. 사람들은 OpenAI가 개방성과 투명성에 대한 초기의 약속을 다시 한 번 후퇴하고 있다고 느꼈습니다. 한 달 후 영리 전환 소식이 전해지면서 연구 보류 소식은 사람들을 더욱 의심하게 만들었습니다. 혹시 향후 라이선스를 취득하기 위해 이 기술을 비밀에 부친 것은 아닐까요?



<figure>
    <img src="https://cdn.technologyreview.com/i/images/cf3a2366final.jpg?sw=700&cx=0&cy=223&cw=1333&ch=1777"
         alt="Ilya Sutskever">
    <figcaption> <br>Ilya Sutskever, co-founder and chief scientist. </br> Christie Hemm Klok </figcaption>
</figure>

---
하지만 OpenAI가 연구 결과를 숨긴 것이 이번만이 아니라는 사실을 아는 사람은 거의 없었습니다. 사실 오픈AI는 또 다른 연구를 완전히 비밀에 부쳤습니다.

AGI에 도달하기 위해 무엇이 필요한지에 대한 두 가지 기술 이론이 널리 퍼져 있습니다. 하나는 필요한 모든 기술이 이미 존재하며, 이를 어떻게 확장하고 조립할지 알아내는 것만 남았다는 이론입니다. 다른 하나는 완전히 새로운 패러다임이 필요하다는 것인데, 현재 AI의 지배적인 기술인 딥러닝만으로는 충분하지 않다는 것입니다.

대부분의 연구자들은 이 두 가지 극단적인 입장 중 어느 쪽에 속하지만, OpenAI는 거의 독점적으로 확장 및 조립 쪽에 속해 있습니다. 대부분의 혁신은 다른 연구실에서 개발된 기술 혁신에 훨씬 더 많은 컴퓨팅 리소스를 투입한 결과였습니다.

브록맨과 서츠케버는 이것이 자신들의 유일한 전략이라고 부인하지만, 연구소의 철저한 보안을 유지하고 있는 연구 결과는 그렇지 않다는 것을 시사합니다. '포사이트'라는 팀은 점점 더 많은 양의 데이터와 컴퓨팅 성능으로 기존 알고리즘을 훈련시켜 AI 기능을 어디까지 발전시킬 수 있는지 테스트하는 실험을 진행합니다. 경영진은 이러한 실험의 결과를 통해 연구소의 컴퓨팅 중심 전략이 최선의 접근 방식이라는 본능을 확인했습니다.

약 6개월 동안 이러한 결과는 외부에 공개되지 않았는데, OpenAI는 이러한 지식이 자사의 주요 경쟁 우위라고 생각했기 때문입니다. 직원과 인턴에게 공개하지 말라고 명시적으로 지시했고, 퇴사한 직원들은 기밀유지 계약서에 서명했습니다. 지난 1월이 되어서야 이 팀은 일반적인 팡파르 없이 조용히 AI 연구를 위한 주요 오픈소스 데이터베이스 중 하나에 논문을 게시했습니다. 이 연구와 관련된 극도의 비밀주의를 경험한 사람들은 이 변화를 어떻게 받아들여야 할지 몰랐습니다. 특히, 몇 달 전에 다른 연구자들이 비슷한 결과를 담은 또 다른 논문이 게시된 적이 있었습니다.


---

처음에는 이러한 수준의 비밀 유지가 의도된 바는 아니었지만, 이후 습관화되었습니다. 시간이 지남에 따라 리더십은 개방성이 유익한 AGI를 구축하는 가장 좋은 방법이라는 원래의 믿음에서 멀어졌습니다. 이제 침묵의 중요성은 연구소에서 함께 일하거나 연구소에서 일하는 사람들에게 깊은 인상을 남겼습니다. 여기에는 커뮤니케이션 팀의 명시적인 허가 없이는 기자에게 절대로 말하지 않는 것도 포함됩니다. 처음 사무실을 방문한 후 다른 직원들과 연락을 취하기 시작했을 때 커뮤니케이션 책임자로부터 모든 인터뷰 요청은 자신을 거쳐야 한다는 이메일을 받았습니다. 제가 사람들이 저에게 하는 말의 신빙성을 떨어뜨릴 수 있다며 거절하자, 그녀는 직원들에게 제가 연락을 취하면 계속 알려주라고 지시했습니다. 기자 출신인 클라크는 나중에 Slack 메시지를 통해 기자가 "주변을 기웃거리고 있다"며 직원들을 칭찬했습니다.

이러한 비밀주의 강화에 대한 성명에서 OpenAI 대변인은 회사 헌장의 한 부분을 다시 언급했습니다. "우리는 안전과 보안 문제로 인해 앞으로 전통적인 출판이 줄어들 것으로 예상한다"며 "안전, 정책 및 표준 연구 공유의 중요성은 더욱 커질 것"이라고 밝혔습니다. 또한 대변인은 "또한 각 릴리스는 이러한 장단점을 평가하기 위해 정보 위험 프로세스를 거치고 있으며, 야생에 출시하기 전에 잠재적인 위험과 영향을 파악하기 위해 결과를 천천히 공개하고자 합니다."라고 덧붙였습니다.

가장 큰 비밀 중 하나는 OpenAI가 다음에 진행 중인 프로젝트입니다. 소식통에 따르면 이 프로젝트는 지난 4년간의 연구의 정점으로, 대규모 컴퓨팅 리소스를 사용해 이미지, 텍스트 및 기타 데이터를 학습하는 AI 시스템이라고 설명했습니다. 초기 작업에는 소규모 팀이 배정되었으며, 다른 팀들도 이 작업에 동참할 것으로 예상됩니다. 이 계획이 전사 회의에서 발표되던 날, 인턴들은 회의에 참석할 수 없었습니다. 이 계획에 대해 잘 아는 사람들은 다음과 같이 설명합니다. 경영진은 이것이 AGI를 달성할 수 있는 가장 유망한 방법이라고 생각합니다.

---

#### 연구 책임자, 다리오 아모데이

OpenAI의 전략을 주도하는 사람은 구글러 출신으로 현재 연구 책임자로 일하고 있는 다리오 아모데이입니다. 처음 만났을 때 그는 브록맨보다 더 불안한 사람으로 보였습니다. 성실함과 감수성은 비슷하지만 불안한 신경 에너지가 느껴집니다. 그는 말을 할 때 미간을 찌푸린 채 무심코 곱슬머리를 잡아당기는 손으로 먼 곳을 응시합니다.

아모데이는 연구소의 전략을 두 부분으로 나눕니다. 첫 번째 부분은 고급 AI 기능에 도달하기 위한 계획으로, 그는 이를 투자자의 "베팅 포트폴리오"에 비유합니다. OpenAI의 각 팀은 서로 다른 베팅을 하고 있습니다. 예를 들어 언어 팀은 AI가 단순한 언어 학습을 통해 세상에 대한 상당한 이해력을 키울 수 있다는 가설에 돈을 걸고 있습니다. 반면 로봇 팀은 지능이 발달하려면 물리적 구현이 필요하다는 반대 이론을 발전시키고 있습니다.

투자자의 포트폴리오에서와 마찬가지로 모든 베팅이 동일한 비중을 가지는 것은 아닙니다. 하지만 과학적 엄밀성을 위해 모든 베팅은 버리기 전에 테스트를 거쳐야 합니다. 아모데이는 놀랍도록 사실적인 자동 생성 텍스트를 제공하는 GPT-2를 열린 마음을 유지하는 것이 중요한 이유를 보여주는 사례로 꼽았습니다. "순수 언어는 현장에서는 물론 우리 중 일부도 다소 회의적이었던 방향입니다."라고 그는 말합니다. "하지만 지금은 '와, 정말 유망하구나'라는 생각이 들었습니다."

시간이 지남에 따라 서로 다른 베팅이 다른 베팅보다 우위를 점하게 되면 더 많은 노력을 기울이게 될 것입니다. 그러면 서로 교차 수분하고 결합할 것입니다. 목표는 점점 더 적은 수의 팀이 궁극적으로 AGI의 단일 기술 방향으로 수렴하는 것입니다. 이것이 바로 OpenAI의 최신 극비 프로젝트가 이미 시작한 것으로 추정되는 정확한 과정입니다.

<figure>
    <img src="https://cdn.technologyreview.com/i/images/cf3a2296final.jpg?sw=700&cx=0&cy=193&cw=1355&ch=1807"
         alt="Dario Amodei">
    <figcaption> <br>Dario Amodei, research director. </br> Christie Hemm Klok </figcaption>
</figure>


아모데이는 이 전략의 두 번째 부분은 이렇게 계속 발전하는 AI 시스템을 안전하게 만드는 방법에 초점을 맞추고 있다고 설명합니다. 여기에는 인간의 가치를 반영하고, 결정의 논리를 설명할 수 있으며, 그 과정에서 사람을 해치지 않고 학습할 수 있도록 하는 것이 포함됩니다. 이러한 각 안전 목표를 전담하는 팀은 프로젝트가 성숙해짐에 따라 프로젝트 전반에 적용할 수 있는 방법을 개발하기 위해 노력합니다. 예를 들어 설명가능성 팀에서 개발한 기술은 GPT-2의 문장 구성이나 로봇의 움직임 뒤에 숨은 논리를 드러내는 데 사용될 수 있습니다.

아모데이는 이 전략의 일부가 이 분야의 확립된 이론에 기반하기보다는 직감에 의존한 다소 우연한 결과물이라고 인정합니다. "언젠가는 AGI를 구축하게 될 텐데, 그때쯤이면 전 세계에서 이러한 시스템이 작동하는 것에 대해 기분이 좋아지고 싶습니다."라고 그는 말합니다. "현재 기분이 좋지 않은 부분이 있다면 그 부분에 집중할 수 있는 팀을 만들고 모집합니다."

홍보와 비밀을 중시하는 아모데이가 이렇게 말할 때면 진심이 느껴집니다. 실패의 가능성은 그를 방해하는 것 같습니다.

"우리는 AGI가 어떤 모습일지 모른다는 어색한 입장에 처해 있습니다."라고 그는 말합니다. "우리는 그것이 언제 일어날지 모릅니다." 그런 다음 그는 조심스럽게 자기 인식을 하면서 이렇게 덧붙입니다: "모든 사람의 마음에는 한계가 있습니다. 제가 찾은 가장 좋은 방법은 제가 생각했던 것과는 다른 비전을 가진 다른 안전 연구원을 고용하는 것이었습니다. 그런 변화와 다양성을 원합니다. 그래야만 모든 것을 파악할 수 있기 때문입니다."

---

문제는 OpenAI에는 실제로 '다양성과 다양성'이 거의 없다는 점입니다. 사무실에 출근한 지 3일째 되던 날 이 사실을 뼈저리게 느꼈습니다. 직원들과 어울릴 수 있는 점심 식사 시간이 주어졌을 때 저는 가장 눈에 띄게 다양한 인종이 모인 테이블에 앉았습니다. 1분도 채 지나지 않아 저는 그곳에서 식사를 하는 사람들이 사실 OpenAI 직원이 아니라는 사실을 깨달았습니다. 컴퓨터와 뇌의 인터페이스를 연구하는 머스크의 스타트업인 뉴럴링크는 같은 건물과 식당을 공유하고 있습니다.

<figure>
    <img src="https://cdn.technologyreview.com/i/images/cf3a2337final.jpg?sw=700&cx=0&cy=0&cw=1333&ch=1777"
         alt="Daniela Amodei">
    <figcaption> <br>Daniela Amodei, head of people operations </br> Christie Hemm Klok </figcaption>
</figure>

연구소 대변인에 따르면 120명이 넘는 직원 중 25%가 여성 또는 논바이너리입니다. 또한 임원진 중 여성은 2명이며 리더십 팀의 여성 비율은 30%라고 그녀는 말했지만, 이 팀에 누가 포함되는지는 구체적으로 밝히지 않았습니다. (브록먼과 알트먼을 포함한 4명의 최고 경영진은 모두 백인 남성입니다. LinkedIn 및 기타 출처에서 확인한 112명 이상의 직원 중 압도적으로 많은 수가 백인이거나 아시아인이었습니다.)

공평하게 말하자면, 이러한 다양성 부족은 AI 업계에서 흔히 볼 수 있는 현상입니다. 지난해 뉴욕에 위치한 연구기관 AI Now의 보고서에 따르면 주요 AI 컨퍼런스의 저자 중 여성은 18%, AI 교수직의 20%, Facebook과 Google의 연구 직원 중 여성은 각각 15%와 10%에 불과한 것으로 나타났습니다. OpenAI의 대변인은 "학계와 업계 전반에 걸쳐 아직 해야 할 일이 많이 남아 있습니다."라고 말했습니다. "다양성과 포용성은 우리가 중요하게 생각하는 부분이며, WiML, 걸 긱, 장학생 프로그램과 같은 이니셔티브와 협력하여 지속적으로 개선하기 위해 노력하고 있습니다."

실제로 OpenAI는 인재 풀을 넓히기 위해 노력해 왔습니다. 2018년에는 소외된 소수민족을 위한 원격 장학생 프로그램을 시작했습니다. 하지만 처음 8명의 장학생 중 단 2명만이 정규직이 되었지만, 이들은 긍정적인 경험을 보고했습니다. 입사 거부의 가장 흔한 이유는 샌프란시스코에 거주해야 한다는 조건이었습니다. 전직 장학생으로 현재 뉴욕에 본사를 둔 회사의 수석 머신러닝 엔지니어로 일하고 있는 나자 로즈에게 샌프란시스코는 다양성이 너무 부족한 도시였습니다.

하지만 다양성이 AI 업계 전반의 문제라면, 기술을 모든 사람에게 골고루 보급하는 것을 사명으로 삼는 기업에게는 더욱 절실한 문제입니다. 소외될 위험이 가장 큰 집단의 대표성이 부족하다는 것이 사실입니다.

또한 브록먼이 자주 언급하는 것처럼 OpenAI가 어떻게 AGI의 혜택을 "모든 인류"에게 "분배"할 계획인지도 명확하지 않습니다. 경영진은 이를 모호한 용어로 표현하고 있으며 구체적인 내용을 거의 밝히지 않고 있습니다. (지난 1월, 옥스퍼드 대학교의 인류의 미래 연구소는 이 연구소와 공동으로 수익의 일정 비율을 분배하여 혜택을 분배할 것을 제안하는 보고서를 발표했습니다. 하지만 저자들은 "이를 구현하는 방식과 관련하여 해결되지 않은 중요한 문제"를 언급했습니다.) 익명을 요구한 한 전직 직원은 "이것이 OpenAI의 가장 큰 문제"라고 말합니다.

<figure>
    <img src="https://cdn.technologyreview.com/i/images/cf3a2401.jpg?sw=700&cx=0&cy=0&cw=3000&ch=2000"
         alt="--">
    <figcaption> <br>DChristie Hemm Klok </figcaption>
</figure>

럿거스 대학의 브릿 패리스는 "인공지능으로 사회 문제에 답하려고 정교한 기술 방식을 사용하고 있습니다."라고 말합니다. "그들은 실제로 사회를 이해할 수 있는 능력이 없는 것 같습니다. 그들은 단지 지금 이 분야가 자신들을 포지셔닝할 수 있는 일종의 수익성 있는 분야라는 것만 알고 있을 뿐입니다."

브록먼은 궁극적으로 OpenAI가 사명을 달성하기 위해서는 기술적 전문성과 사회적 전문성이 모두 필요하다는 데 동의합니다. 하지만 그는 처음부터 사회적 문제를 해결해야 한다는 데는 동의하지 않습니다. "윤리를 비롯한 다른 관점을 정확히 어떻게 도입할 것인가? 그리고 언제 어떻게 도입해야 할까요? 추구할 수 있는 한 가지 전략은 처음부터 필요한 모든 것을 구워 넣는 것입니다."라고 그는 말합니다. "저는 이 전략이 성공할 가능성은 없다고 생각합니다."

그는 가장 먼저 알아내야 할 것은 AGI가 어떤 모습일지 파악하는 것이라고 말합니다. 그런 다음에는 "우리가 그 파급 효과를 제대로 이해하고 있는지 확인"해야 합니다.

---


지난 여름(2019년), 수익 제한 모델로 전환하고 Microsoft로부터 10억 달러를 투자받은 후 몇 주 동안 경영진은 직원들에게 이러한 업데이트가 OpenAI의 연구 접근 방식을 기능적으로 변화시키지 않을 것이라고 확신했습니다. Microsoft는 연구소의 가치와 잘 맞아떨어지며, 상업화 노력은 먼 훗날의 일이고 근본적인 질문의 추구는 여전히 연구 활동의 핵심으로 남을 것이라는 것이었습니다.

한동안은 이러한 확신이 사실인 것처럼 보였고 프로젝트는 그대로 진행되었습니다. 많은 직원들은 Microsoft가 어떤 약속을 했는지, 있다면 어떤 약속을 했는지조차 알지 못했습니다.

하지만 최근 몇 달 동안 상업화에 대한 압박이 심해졌고, 돈을 버는 연구를 만들어야 한다는 필요성이 더 이상 먼 미래의 일처럼 느껴지지 않았습니다. 2020년 연구소의 비전을 직원들과 비공개로 공유하면서 알트먼이 전한 메시지는 분명했습니다. 연구를 하려면 돈을 벌어야지, 그 반대가 되어서는 안 된다는 것이었습니다.

부유한 자선 기부자가 부족하기 때문에 어쩔 수 없이 감수해야 하는 어렵지만 필수적인 절충안이라고 경영진은 말합니다. 반면, 시애틀에 본사를 둔 비영리 단체인 AI2는 야심차게 기초 AI 연구를 발전시키고 있으며, 마이크로소프트 공동 창업자로 잘 알려진 억만장자 고 폴 앨런이 남긴 자립형(적어도 당분간은) 자금 풀에서 자금을 지원받습니다.

하지만 사실 OpenAI는 부자가 아니기 때문이기도 하지만, 다른 누구보다 먼저 AGI에 도달하려는 전략적 선택을 했기 때문에 이러한 상충 관계에 직면해 있습니다. 이러한 압박으로 인해 원래 의도와는 점점 더 멀어지는 결정을 내릴 수밖에 없습니다. 자금과 인재를 유치하기 위해 서둘러 과대광고에 의존하고, 우위를 점하기 위해 연구를 보호하며, AGI에 이르는 유일한 방법이라고 생각해서가 아니라 가장 빠를 것 같다는 이유로 계산량이 많은 전략을 추구합니다.

하지만 OpenAI는 여전히 인류를 위해 진심으로 노력하는 사람들로 가득 찬 인재와 최첨단 연구의 보루입니다. 다시 말해, 여전히 가장 중요한 요소들을 갖추고 있으며, 아직 변화할 수 있는 시간이 남아 있습니다.

전직 원격 학자였던 로즈와의 인터뷰가 끝날 무렵, 저는 이 프로필에서 빼놓지 말아야 할 OpenAI에 대한 한 가지를 물었습니다. "제 생각에는 문제가 있는 것 같아요." 로즈가 머뭇거리며 말을 시작합니다. "그 중 몇 가지는 OpenAI가 직면한 환경에서 비롯된 것이고, 어떤 사람들은 끌어들이는 경향이 있고 어떤 사람들은 배제하는 경향이 있습니다."

"하지만 제게는 그들이 뭔가 조금은 제대로 하고 있는 것 같습니다."라고 그녀는 말합니다. "그곳에 있는 사람들이 진지하게 노력하고 있다는 느낌을 받았습니다."

업데이트: OpenAI에서 그렉 브록먼이 AI를 개발할 때 "처음부터 윤리를 넣는 것은 불가능하다고 생각한다"고 말한 것은 처음부터 윤리적 문제를 해결할 수 없다는 뜻이 아니라 처음부터 윤리적 문제를 다룰 수 없다는 뜻이라는 점을 명확히 해달라는 요청에 따라 이 이야기를 일부 수정했습니다. 또한 하버드를 중퇴한 후 1년을 기다리지 않고 바로 MIT로 편입한 것도 마찬가지입니다. 또한 그는 "농장에서"가 아니라 "취미 농장에서" 자랐습니다. 브록먼은 이 구분이 중요하다고 생각합니다.

또한, OpenAI가 실제로 "비영리 단체의 지위를 내려놓긴 했지만" 여전히 비영리 단체의 일부인 이사회가 이를 감독하고 있으며, 연구 논문을 대신하는 것이 아니라 회사 블로그 게시물 형태로 연구를 발표하고 있다는 점을 명확히 했습니다. 또한 외부 연구자의 논문 발표 날짜와 피터 에커슬리(현재가 아닌 전, 최근 퇴사한 파트너십 온 AI의 연구 책임자)의 소속도 정정했습니다.




#### 참고 : OpenAI(from [Wikipedia](https://en.wikipedia.org/wiki/OpenAI))

The organization was founded in December 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial board members.[^9] [^10] [^11] Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023,[^12] [^13] with a significant portion of the investment in the form of compute resources on Microsoft's Azure cloud service.[^14]
    
On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned. The new initial board included former Salesforce co-CEO Bret Taylor as chairman.[^15]

 [^9]:"Sam Altman on His Plan to Keep A.I. Out of the Hands of the "Bad Guys"". Vanity Fair. 2015. Archived from the original on February 3, 2023. Retrieved January 23, 2023.
 
 [^10]:"Introducing OpenAI". OpenAI. December 12, 2015. Archived from the original on August 8, 2017. Retrieved January 27, 2023.
 
 [^11]:"OpenAI, the company behind ChatGPT: What all it does, how it started and more". The Times of India. January 25, 2023. Archived from the original on February 3, 2023. Retrieved January 28, 2023.
 
 [^12]:Browne, Ryan (January 10, 2023). "Microsoft reportedly plans to invest $10 billion in creator of buzzy A.I. tool ChatGPT". CNBC. Archived from the original on February 3, 2023. Retrieved January 27, 2023.
 
 [^13]:Lardinois, Frederic (March 14, 2023). "Microsoft's new Bing was using GPT-4 all along". TechCrunch. Archived from the original on March 15, 2023. Retrieved March 30, 2023.
 
 [^14]:"OpenAI has received just a fraction of Microsoft's $10 billion investment". Semafor. November 18, 2023. Retrieved November 27, 2023.

 [^15]:"OpenAI brings Sam Altman back as CEO less than a week after he was fired by board". CNBC. November 22, 2023. -->